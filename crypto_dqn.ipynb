{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrU72DTN4PUhjUB5ItaYCq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker0803r/RL/blob/main/crypto_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝 SDK"
      ],
      "metadata": {
        "id": "0Vm3bNo2hvtU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "5vkA_b_ghsQi"
      },
      "outputs": [],
      "source": [
        "#!pip install gate-api pandas numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 導入套件"
      ],
      "metadata": {
        "id": "UGfzDj4_i7N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "zRXLiZYgiaOd"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GateioDataFetcher"
      ],
      "metadata": {
        "id": "4b3bSnM9h0zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gate_api import Configuration, ApiClient, SpotApi\n",
        "\n",
        "def fetch_gateio_klines(pair: str, interval: str, limit: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    使用 Gate.io SDK 獲取 K 線數據。\n",
        "\n",
        "    :param pair: 交易對, 例如 'BTC_USDT'\n",
        "    :param interval: K 線間隔, 例如 '1d', '4h', '1h', '30m'\n",
        "    :param limit: 獲取數據的數量上限 (Gate.io 限制單次最大為 1000)\n",
        "    :return: 包含 OHLCV 數據的 Pandas DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # 設置 API Client\n",
        "    # 注意：獲取公共市場數據不需要 API Key 和 Secret，但為了通用性可以配置。\n",
        "    # 這裡使用默認配置，僅用於獲取公共數據。\n",
        "    config = Configuration()\n",
        "    client = ApiClient(config)\n",
        "    spot_api = SpotApi(client)\n",
        "\n",
        "    try:\n",
        "        # 呼叫 API\n",
        "        # Gate.io 的 K 線數據返回格式為 [時間戳, 交易量, 結算貨幣交易量, 收盤價, 最高價, 最低價, 開盤價]\n",
        "        klines = spot_api.list_candlesticks(currency_pair=pair, interval=interval, limit=limit)\n",
        "\n",
        "        # 將結果轉換為 DataFrame\n",
        "        data = []\n",
        "        for k in klines:\n",
        "            # 確保數據的順序和類型\n",
        "            # [time, volume, close_volume, close, high, low, open]\n",
        "            data.append([\n",
        "                float(k[0]), # Time (時間戳)\n",
        "                float(k[6]), # Open\n",
        "                float(k[4]), # High\n",
        "                float(k[5]), # Low\n",
        "                float(k[3]), # Close\n",
        "                float(k[1]), # Volume\n",
        "            ])\n",
        "\n",
        "        df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "        # 設置時間索引並進行整理\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "        df = df.set_index('timestamp')\n",
        "        df = df.sort_index() # 確保時間順序是遞增的 (舊 -> 新)\n",
        "\n",
        "        # 增加必要的技術指標（這裡只是一個簡單的價格變動）\n",
        "        df['price_change'] = df['close'].diff().fillna(0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"從 Gate.io 獲取數據失敗: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "fetch_gateio_klines('XRP_USDT','15m').head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "KqBNZZmbhxj-",
        "outputId": "02dd975c-42bc-441b-cb7e-0fcc94388c53"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           open   high    low  close        volume  \\\n",
              "timestamp                                                            \n",
              "2025-11-19 07:15:00  585724.186  2.152  2.157  2.159  1.262017e+06   \n",
              "2025-11-19 07:30:00  575053.749  2.152  2.154  2.164  1.241721e+06   \n",
              "2025-11-19 07:45:00  753490.433  2.162  2.163  2.171  1.633748e+06   \n",
              "2025-11-19 08:00:00  555375.205  2.163  2.168  2.170  1.203171e+06   \n",
              "2025-11-19 08:15:00  679393.654  2.151  2.165  2.165  1.467478e+06   \n",
              "\n",
              "                     price_change  \n",
              "timestamp                          \n",
              "2025-11-19 07:15:00         0.000  \n",
              "2025-11-19 07:30:00         0.005  \n",
              "2025-11-19 07:45:00         0.007  \n",
              "2025-11-19 08:00:00        -0.001  \n",
              "2025-11-19 08:15:00        -0.005  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6aa5a2f7-eae3-4695-8ee2-ef5bd44fd871\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>price_change</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-11-19 07:15:00</th>\n",
              "      <td>585724.186</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.157</td>\n",
              "      <td>2.159</td>\n",
              "      <td>1.262017e+06</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-11-19 07:30:00</th>\n",
              "      <td>575053.749</td>\n",
              "      <td>2.152</td>\n",
              "      <td>2.154</td>\n",
              "      <td>2.164</td>\n",
              "      <td>1.241721e+06</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-11-19 07:45:00</th>\n",
              "      <td>753490.433</td>\n",
              "      <td>2.162</td>\n",
              "      <td>2.163</td>\n",
              "      <td>2.171</td>\n",
              "      <td>1.633748e+06</td>\n",
              "      <td>0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-11-19 08:00:00</th>\n",
              "      <td>555375.205</td>\n",
              "      <td>2.163</td>\n",
              "      <td>2.168</td>\n",
              "      <td>2.170</td>\n",
              "      <td>1.203171e+06</td>\n",
              "      <td>-0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-11-19 08:15:00</th>\n",
              "      <td>679393.654</td>\n",
              "      <td>2.151</td>\n",
              "      <td>2.165</td>\n",
              "      <td>2.165</td>\n",
              "      <td>1.467478e+06</td>\n",
              "      <td>-0.005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6aa5a2f7-eae3-4695-8ee2-ef5bd44fd871')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6aa5a2f7-eae3-4695-8ee2-ef5bd44fd871 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6aa5a2f7-eae3-4695-8ee2-ef5bd44fd871');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b3413f9e-6993-4a19-8ac4-65b3fd4c1060\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3413f9e-6993-4a19-8ac4-65b3fd4c1060')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b3413f9e-6993-4a19-8ac4-65b3fd4c1060 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"fetch_gateio_klines('XRP_USDT','15m')\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-11-19 07:15:00\",\n        \"max\": \"2025-11-19 08:15:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-11-19 07:30:00\",\n          \"2025-11-19 08:15:00\",\n          \"2025-11-19 07:45:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 84020.43344439928,\n        \"min\": 555375.205,\n        \"max\": 753490.433,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          575053.749,\n          679393.654,\n          753490.433\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"high\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0059581876439064145,\n        \"min\": 2.151,\n        \"max\": 2.163,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.162,\n          2.151,\n          2.152\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005770615218501461,\n        \"min\": 2.154,\n        \"max\": 2.168,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.154,\n          2.165,\n          2.163\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004868264577855223,\n        \"min\": 2.159,\n        \"max\": 2.171,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.164,\n          2.165,\n          2.171\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 183481.77817401558,\n        \"min\": 1203171.359086,\n        \"max\": 1633748.158154,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1241720.880952,\n          1467477.894761,\n          1633748.158154\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_change\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004816637831516839,\n        \"min\": -0.004999999999999893,\n        \"max\": 0.006999999999999673,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0050000000000003375,\n          -0.004999999999999893,\n          0.006999999999999673\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GateioTradingEnv"
      ],
      "metadata": {
        "id": "Y_DDird5iDkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class GateioTradingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    基於 Gate.io K 線數據的 RL 交易環境 (MDP 格式)。\n",
        "\n",
        "    狀態 (State): 過去 N 根 K 線數據 + 當前賬戶狀態\n",
        "    動作 (Action): 離散的 (買入/賣出/持有)\n",
        "    獎勵 (Reward): 淨值變動\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {'render_modes': ['human'], 'render_fps': 1}\n",
        "\n",
        "    # 定義動作：離散空間\n",
        "    ACTION_HOLD = 0\n",
        "    ACTION_BUY = 1\n",
        "    ACTION_SELL = 2\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, window_size: int = 15, initial_balance: float = 10000.0, fee_rate: float = 0.00075):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.initial_balance = initial_balance\n",
        "        self.fee_rate = fee_rate # 交易手續費率 (Gate.io 現貨 VIP0 約 0.075%)\n",
        "\n",
        "        self.current_step = self.window_size # 從第 window_size 步開始回測\n",
        "\n",
        "        # --- 動作空間 (Action Space) ---\n",
        "        # 0: Hold (保持/空倉), 1: Buy (全倉買入), 2: Sell (全倉賣出)\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # --- 狀態空間 (Observation Space) ---\n",
        "        # 狀態維度 = window_size * (OHLCV + 指標數) + 賬戶狀態數\n",
        "        N_FEATURES = 6 # (Open, High, Low, Close, Volume, Price_Change)\n",
        "        N_ACCOUNT_VARS = 2 # (持倉量, 當前淨值)\n",
        "\n",
        "        # 狀態是 Box 類型\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(self.window_size * N_FEATURES + N_ACCOUNT_VARS,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # 賬戶變數初始化 (在 reset 中完成)\n",
        "        self.balance = 0.0      # 當前持有的 USD/USDT\n",
        "        self.crypto_held = 0.0  # 當前持有的交易對基礎幣 (例如 BTC)\n",
        "        self.net_worth = 0.0    # 淨值 = balance + crypto_held * current_price\n",
        "        self.last_net_worth = 0.0 # 用於計算獎勵\n",
        "        self.trade_history = [] # 交易紀錄 (可選)\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        獲取當前時間步 t 的狀態 s_t。\n",
        "        \"\"\"\n",
        "        # 1. 提取 K 線數據 (過去 window_size 根)\n",
        "        start = self.current_step - self.window_size\n",
        "        end = self.current_step\n",
        "\n",
        "        # 提取 OHLCV + 指標\n",
        "        window_data = self.df.iloc[start:end, :]\n",
        "\n",
        "        # 確保數據形狀正確\n",
        "        market_features = window_data.values.flatten()\n",
        "\n",
        "        # 2. 獲取當前價格 (用作淨值計算)\n",
        "        current_price = self.df.iloc[self.current_step - 1]['close']\n",
        "\n",
        "        # 3. 更新當前淨值 (用當前價格計算)\n",
        "        self.net_worth = self.balance + self.crypto_held * current_price\n",
        "\n",
        "        # 4. 賬戶狀態特徵\n",
        "        account_state = np.array([\n",
        "            self.crypto_held,\n",
        "            self.net_worth / self.initial_balance # 淨值比例 (歸一化)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # 5. 合併為最終狀態\n",
        "        observation = np.concatenate([market_features, account_state])\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        重設環境到初始狀態。\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # 隨機選擇起始點，確保有足夠的 window_size 歷史數據\n",
        "        self.current_step = self.window_size + random.randint(0, len(self.df) - self.window_size - 1)\n",
        "\n",
        "        # 交易變數重設\n",
        "        self.balance = self.initial_balance\n",
        "        self.crypto_held = 0.0\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.last_net_worth = self.initial_balance\n",
        "        self.trade_history = []\n",
        "\n",
        "        observation = self._get_observation()\n",
        "        info = {'net_worth': self.net_worth, 'step': self.current_step}\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def _take_action(self, action: int):\n",
        "        \"\"\"\n",
        "        執行動作並更新賬戶狀態。\n",
        "        \"\"\"\n",
        "        current_price = self.df.iloc[self.current_step - 1]['close']\n",
        "        trade_amount = 0.0 # 交易量\n",
        "\n",
        "        if action == self.ACTION_BUY and self.balance > 0:\n",
        "            # 全倉買入 (假設不考慮最小交易量限制)\n",
        "\n",
        "            # 可用來購買的 USDT/USD 數量\n",
        "            buy_power = self.balance / (1 + self.fee_rate)\n",
        "\n",
        "            # 購買的幣數量\n",
        "            amount_to_buy = buy_power / current_price\n",
        "\n",
        "            fee = self.balance - buy_power # 手續費\n",
        "\n",
        "            self.crypto_held += amount_to_buy\n",
        "            self.balance = 0.0 # 餘額清零\n",
        "\n",
        "            trade_amount = amount_to_buy\n",
        "            self.trade_history.append((self.current_step, 'BUY', trade_amount, current_price, fee))\n",
        "\n",
        "        elif action == self.ACTION_SELL and self.crypto_held > 0:\n",
        "            # 全倉賣出\n",
        "            amount_to_sell = self.crypto_held\n",
        "\n",
        "            # 獲得的 USDT/USD 數量 (扣除手續費)\n",
        "            proceeds = amount_to_sell * current_price * (1 - self.fee_rate)\n",
        "\n",
        "            fee = amount_to_sell * current_price * self.fee_rate\n",
        "\n",
        "            self.balance += proceeds\n",
        "            self.crypto_held = 0.0 # 持倉清零\n",
        "\n",
        "            trade_amount = amount_to_sell\n",
        "            self.trade_history.append((self.current_step, 'SELL', trade_amount, current_price, fee))\n",
        "\n",
        "        # ACTION_HOLD (0) 不執行任何交易，保持當前狀態\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        執行一個時間步。\n",
        "        :param action: Agent 選擇的動作 (0, 1, 2)\n",
        "        :return: (observation, reward, terminated, truncated, info)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. 執行動作 (更新 self.balance, self.crypto_held)\n",
        "        self._take_action(action)\n",
        "\n",
        "        # 2. 推進時間\n",
        "        self.current_step += 1\n",
        "\n",
        "        # 3. 獲取新狀態 s' (並同時計算當前淨值 self.net_worth)\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # 檢查是否到達數據結尾\n",
        "        if self.current_step >= len(self.df):\n",
        "            terminated = True\n",
        "            # 最後一步強制平倉計算最終淨值\n",
        "            if self.crypto_held > 0:\n",
        "                 self._take_action(self.ACTION_SELL)\n",
        "\n",
        "            observation = self._get_observation()\n",
        "\n",
        "        else:\n",
        "            observation = self._get_observation()\n",
        "\n",
        "        # 4. 計算獎勵 (Reward)\n",
        "\n",
        "        # 獎勵 = 淨值變化率\n",
        "        # 最小化負值，最大化正值\n",
        "        reward = (self.net_worth - self.last_net_worth) / self.last_net_worth\n",
        "\n",
        "        # 更新上一步淨值\n",
        "        self.last_net_worth = self.net_worth\n",
        "\n",
        "        # 5. 檢查結束條件 (Termination/Truncation)\n",
        "\n",
        "        # 簡易爆倉/虧損清算 (可選)\n",
        "        if self.net_worth < self.initial_balance * 0.5: # 虧損超過 50% 強制結束\n",
        "             terminated = True\n",
        "             reward = -10.0 # 爆倉給予極大懲罰\n",
        "\n",
        "\n",
        "        info = {\n",
        "            'net_worth': self.net_worth,\n",
        "            'crypto_held': self.crypto_held,\n",
        "            'balance': self.balance,\n",
        "            'reward': reward,\n",
        "            'step': self.current_step\n",
        "        }\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"\n",
        "        環境可視化 (簡單輸出)。\n",
        "        \"\"\"\n",
        "        if mode == 'human':\n",
        "            print(f\"Step: {self.current_step} | Net Worth: ${self.net_worth:,.2f} | Crypto Held: {self.crypto_held:.4f} | Action: {self.action_space.names[self.action]}\")\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        清理資源 (例如關閉可視化窗口)。\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "t_cqmfXgh-H-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 運行與測試 (使用真實 API 數據)"
      ],
      "metadata": {
        "id": "CigI7Yj8i4SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # --- 1. 數據獲取配置 ---\n",
        "    TARGET_PAIR = 'BTC_USDT'\n",
        "    INTERVAL = '4h' # 4 小時 K 線\n",
        "    DATA_LIMIT = 500 # 獲取最近 500 條 K 線\n",
        "    WINDOW_SIZE = 30\n",
        "\n",
        "    print(f\"--- 1. 嘗試從 Gate.io 獲取 {TARGET_PAIR} 的 K 線數據 ({DATA_LIMIT} 條, {INTERVAL} 間隔) ---\")\n",
        "\n",
        "    # 呼叫真實的 API 數據獲取函數\n",
        "    df_data = fetch_gateio_klines(TARGET_PAIR, INTERVAL, DATA_LIMIT)\n",
        "\n",
        "    if df_data.empty or len(df_data) < WINDOW_SIZE:\n",
        "        print(\"數據不足或獲取失敗，無法創建環境。\")\n",
        "    else:\n",
        "        print(f\"成功獲取數據。數據長度: {len(df_data)} 條 K 線\")\n",
        "\n",
        "        # --- 2. 創建環境 ---\n",
        "        ENV_CONFIG = {\n",
        "            'df': df_data,\n",
        "            'window_size': WINDOW_SIZE,\n",
        "            'initial_balance': 10000.0,\n",
        "            'fee_rate': 0.00075\n",
        "        }\n",
        "\n",
        "        env = GateioTradingEnv(**ENV_CONFIG)\n",
        "\n",
        "        # ... (後續的運行和測試邏輯與前一個回答相同) ...\n",
        "        # 3. 運行一個 Episode\n",
        "        print(\"\\n--- 3. 運行一個 Episode ---\")\n",
        "\n",
        "        observation, info = env.reset()\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        total_reward = 0.0\n",
        "\n",
        "        print(f\"起始淨值: ${info['net_worth']:,.2f}\")\n",
        "\n",
        "        while not terminated and not truncated:\n",
        "            action = env.action_space.sample() # 隨機動作\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            if env.current_step % 100 == 0 or terminated:\n",
        "                print(f\"Step {env.current_step}/{len(df_data)} | Net Worth: ${info['net_worth']:,.2f} | Last Reward: {reward:.4f}\")\n",
        "\n",
        "        print(\"\\n--- 4. Episode 結束 ---\")\n",
        "        final_net_worth = info['net_worth']\n",
        "        profit_rate = (final_net_worth / env.initial_balance - 1) * 100\n",
        "\n",
        "        print(f\"最終淨值: ${final_net_worth:,.2f}\")\n",
        "        print(f\"總報酬率: {profit_rate:.2f}%\")\n",
        "        env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQKSNOi8iR1t",
        "outputId": "c8e8b964-097b-49be-e208-15278e08dc08"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 嘗試從 Gate.io 獲取 BTC_USDT 的 K 線數據 (500 條, 4h 間隔) ---\n",
            "成功獲取數據。數據長度: 500 條 K 線\n",
            "\n",
            "--- 3. 運行一個 Episode ---\n",
            "起始淨值: $10,000.00\n",
            "Step 400/500 | Net Worth: $8,942.49 | Last Reward: 0.0060\n",
            "Step 500/500 | Net Worth: $7,949.07 | Last Reward: -0.0009\n",
            "\n",
            "--- 4. Episode 結束 ---\n",
            "最終淨值: $7,949.07\n",
            "總報酬率: -20.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q Learning\n"
      ],
      "metadata": {
        "id": "o8xDVoKVlzty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 設置計算設備\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用的設備: {DEVICE}\")\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    深度 Q 網絡 (DQN)\n",
        "    輸入: 狀態向量 (Observation Space shape)\n",
        "    輸出: 每個動作的 Q 值 (Action Space size)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        # 定義一個簡單的多層感知機 (MLP)\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float().to(DEVICE) # 確保輸入是 float 且在正確的設備上\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    經驗回放緩衝區\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"添加經驗: (s, a, r, s', done)\"\"\"\n",
        "        # 將 numpy 轉換為 list 或 tuple\n",
        "        state = state.tolist() if isinstance(state, np.ndarray) else state\n",
        "        next_state = next_state.tolist() if isinstance(next_state, np.ndarray) else next_state\n",
        "\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"從緩衝區中隨機採樣 batch_size 個經驗\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return None\n",
        "\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "        # 轉換為 PyTorch Tensor\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32, device=DEVICE)\n",
        "        actions = torch.tensor(actions, dtype=torch.long, device=DEVICE)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=DEVICE)\n",
        "        # Done 標誌需要是浮點數以進行乘法運算\n",
        "        dones = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, gamma, lr, buffer_capacity, batch_size, target_update_freq):\n",
        "\n",
        "        self.gamma = gamma          # 折扣因子\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "        self.action_dim = action_dim\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        # 初始化 Q 網絡和目標 Q 網絡\n",
        "        self.q_net = QNetwork(state_dim, action_dim).to(DEVICE)\n",
        "        self.target_net = QNetwork(state_dim, action_dim).to(DEVICE)\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict()) # 初始同步\n",
        "        self.target_net.eval() # 目標網絡只用於預測，設為評估模式\n",
        "\n",
        "        # 初始化優化器和經驗回放\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
        "        self.buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "        # 探索參數\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9995 # 每次動作後的衰減率\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Epsilon-Greedy 策略選擇動作\"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim) # 探索\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                # 網絡預測 Q 值\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "                q_values = self.q_net(state_tensor)\n",
        "                return q_values.argmax(dim=1).item() # 利用\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"訓練 Q 網絡\"\"\"\n",
        "\n",
        "        # 經驗回放緩衝區不足，不訓練\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # 1. 從緩衝區採樣經驗\n",
        "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        # 2. 計算當前 Q 值 Q(s, a)\n",
        "        # self.q_net(states) 得到所有動作的 Q 值，然後用 gather 提取實際動作的 Q 值\n",
        "        q_current = self.q_net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # 3. 計算目標 Q 值 R + γ * max Q'(s', a')\n",
        "\n",
        "        # 獲取下一狀態的目標 Q 值 (使用 Target Network)\n",
        "        # target_net.max(1)[0] 找到每個下一狀態的最高 Q 值\n",
        "        with torch.no_grad():\n",
        "            q_next_target = self.target_net(next_states).max(1)[0]\n",
        "\n",
        "            # 如果 Episode 結束 (done=1)，則目標 Q 值只剩 R (因為 Q'(s', a') = 0)\n",
        "            q_target = rewards + self.gamma * q_next_target * (1 - dones)\n",
        "\n",
        "        # 4. 計算損失 (L = MSE(Q_current, Q_target))\n",
        "        loss = F.mse_loss(q_current, q_target)\n",
        "\n",
        "        # 5. 優化\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # 6. 軟更新 epsilon (探索率)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # 7. 硬更新目標網絡\n",
        "        self.learn_step_counter += 1\n",
        "        if self.learn_step_counter % self.target_update_freq == 0:\n",
        "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.q_net.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.q_net.load_state_dict(torch.load(path))\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "def train_dqn(env, agent, total_episodes, warm_up_steps):\n",
        "\n",
        "    # 訓練參數\n",
        "    MAX_STEPS_PER_EPISODE = len(env.df) - env.window_size # 最大步數\n",
        "\n",
        "    print(f\"\\n--- 訓練參數 ---\")\n",
        "    print(f\"總 Episode 數: {total_episodes}\")\n",
        "    print(f\"環境每 Episode 最大步數: {MAX_STEPS_PER_EPISODE}\")\n",
        "    print(f\"暖機步數 (Warm-up Steps): {warm_up_steps}\")\n",
        "    print(f\"------------------\")\n",
        "\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(1, total_episodes + 1):\n",
        "\n",
        "        # 1. 重設環境\n",
        "        state, info = env.reset()\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # 2. 運行 Episode\n",
        "        while not terminated and not truncated:\n",
        "\n",
        "            # 3. Agent 選擇動作\n",
        "            # 由於是單環境，直接使用 agent.choose_action\n",
        "            action = agent.choose_action(state)\n",
        "\n",
        "            # 4. 環境推進\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            # 5. 存儲經驗到回放緩衝區\n",
        "            agent.buffer.push(state, action, reward, next_state, terminated)\n",
        "\n",
        "            # 6. 訓練 (暖機步數後才開始訓練)\n",
        "            if len(agent.buffer) >= warm_up_steps:\n",
        "                agent.learn()\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if step_count >= MAX_STEPS_PER_EPISODE:\n",
        "                truncated = True\n",
        "\n",
        "        # 7. 記錄和報告\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        if episode % 10 == 0 or episode == 1:\n",
        "            avg_net_worth = info['net_worth']\n",
        "            epsilon = agent.epsilon * 100\n",
        "            print(f\"Episode: {episode}/{total_episodes} | Steps: {step_count} | Avg Reward: {np.mean(episode_rewards[-10:]):.4f} | Final Net Worth: ${avg_net_worth:,.2f} | Epsilon: {epsilon:.2f}%\")\n",
        "\n",
        "    # 8. 保存最終模型\n",
        "    model_save_path = \"./gateio_dqn_model.pth\"\n",
        "    agent.save(model_save_path)\n",
        "    print(f\"\\nDQN 訓練完成，模型已保存至 {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJh-Vhwqno0x",
        "outputId": "a36ce127-6b54-4920-ed17-0a6257f98f05"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用的設備: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練"
      ],
      "metadata": {
        "id": "kmmdhsfaoGqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    df_data = fetch_gateio_klines('BTC_USDT', '4h', 500)\n",
        "    if df_data.empty or len(df_data) < 30:\n",
        "        print(\"數據不足，無法進行訓練。\")\n",
        "    else:\n",
        "        # --- 1. 環境初始化 ---\n",
        "        ENV_CONFIG = {\n",
        "            'df': df_data,\n",
        "            'window_size': 30,\n",
        "            'initial_balance': 10000.0,\n",
        "            'fee_rate': 0.00075\n",
        "        }\n",
        "        env = GateioTradingEnv(**ENV_CONFIG)\n",
        "\n",
        "        # --- 2. Agent 初始化 ---\n",
        "        STATE_DIM = env.observation_space.shape[0]\n",
        "        ACTION_DIM = env.action_space.n\n",
        "\n",
        "        AGENT_CONFIG = {\n",
        "            'state_dim': STATE_DIM,\n",
        "            'action_dim': ACTION_DIM,\n",
        "            'gamma': 0.99,                  # 折扣因子\n",
        "            'lr': 1e-4,                     # 學習率\n",
        "            'buffer_capacity': 10000,       # 經驗回放緩衝區容量\n",
        "            'batch_size': 64,               # 訓練批次大小\n",
        "            'target_update_freq': 1000      # 目標網絡更新頻率 (每 1000 步更新一次)\n",
        "        }\n",
        "\n",
        "        agent = DQNAgent(**AGENT_CONFIG)\n",
        "\n",
        "        # --- 3. 啟動訓練 ---\n",
        "        TOTAL_EPISODES = 500\n",
        "        # 緩衝區需要填滿到至少 batch_size，但通常設一個較大的值\n",
        "        WARM_UP_STEPS = 2000\n",
        "\n",
        "        train_dqn(env, agent, TOTAL_EPISODES, WARM_UP_STEPS)\n",
        "\n",
        "        env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyOhJhIpn859",
        "outputId": "eda38391-4877-4822-e293-43298ccea845"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 訓練參數 ---\n",
            "總 Episode 數: 500\n",
            "環境每 Episode 最大步數: 470\n",
            "暖機步數 (Warm-up Steps): 2000\n",
            "------------------\n",
            "Episode: 1/500 | Steps: 402 | Avg Reward: -0.2910 | Final Net Worth: $7,417.55 | Epsilon: 100.00%\n",
            "Episode: 10/500 | Steps: 399 | Avg Reward: -0.1328 | Final Net Worth: $8,035.25 | Epsilon: 83.19%\n",
            "Episode: 20/500 | Steps: 427 | Avg Reward: -0.2088 | Final Net Worth: $6,680.51 | Epsilon: 17.82%\n",
            "Episode: 30/500 | Steps: 357 | Avg Reward: -0.2145 | Final Net Worth: $8,817.40 | Epsilon: 4.72%\n",
            "Episode: 40/500 | Steps: 304 | Avg Reward: -0.1529 | Final Net Worth: $8,344.16 | Epsilon: 1.23%\n",
            "Episode: 50/500 | Steps: 127 | Avg Reward: -0.1550 | Final Net Worth: $9,080.18 | Epsilon: 1.00%\n",
            "Episode: 60/500 | Steps: 175 | Avg Reward: -0.1659 | Final Net Worth: $7,712.16 | Epsilon: 1.00%\n",
            "Episode: 70/500 | Steps: 331 | Avg Reward: -0.1401 | Final Net Worth: $7,979.49 | Epsilon: 1.00%\n",
            "Episode: 80/500 | Steps: 220 | Avg Reward: -0.1162 | Final Net Worth: $8,835.21 | Epsilon: 1.00%\n",
            "Episode: 90/500 | Steps: 11 | Avg Reward: -0.1244 | Final Net Worth: $10,014.69 | Epsilon: 1.00%\n",
            "Episode: 100/500 | Steps: 238 | Avg Reward: -0.0823 | Final Net Worth: $8,494.68 | Epsilon: 1.00%\n",
            "Episode: 110/500 | Steps: 124 | Avg Reward: -0.1660 | Final Net Worth: $9,126.51 | Epsilon: 1.00%\n",
            "Episode: 120/500 | Steps: 439 | Avg Reward: -0.1261 | Final Net Worth: $8,515.34 | Epsilon: 1.00%\n",
            "Episode: 130/500 | Steps: 68 | Avg Reward: -0.1446 | Final Net Worth: $10,061.08 | Epsilon: 1.00%\n",
            "Episode: 140/500 | Steps: 214 | Avg Reward: -0.1331 | Final Net Worth: $8,920.18 | Epsilon: 1.00%\n",
            "Episode: 150/500 | Steps: 23 | Avg Reward: -0.0643 | Final Net Worth: $10,245.05 | Epsilon: 1.00%\n",
            "Episode: 160/500 | Steps: 365 | Avg Reward: -0.0699 | Final Net Worth: $9,005.70 | Epsilon: 1.00%\n",
            "Episode: 170/500 | Steps: 267 | Avg Reward: -0.2299 | Final Net Worth: $6,879.06 | Epsilon: 1.00%\n",
            "Episode: 180/500 | Steps: 107 | Avg Reward: -0.1276 | Final Net Worth: $8,710.50 | Epsilon: 1.00%\n",
            "Episode: 190/500 | Steps: 386 | Avg Reward: -0.1260 | Final Net Worth: $8,283.39 | Epsilon: 1.00%\n",
            "Episode: 200/500 | Steps: 67 | Avg Reward: -0.1306 | Final Net Worth: $9,737.11 | Epsilon: 1.00%\n",
            "Episode: 210/500 | Steps: 306 | Avg Reward: -0.1191 | Final Net Worth: $7,868.04 | Epsilon: 1.00%\n",
            "Episode: 220/500 | Steps: 25 | Avg Reward: -0.1402 | Final Net Worth: $10,225.48 | Epsilon: 1.00%\n",
            "Episode: 230/500 | Steps: 380 | Avg Reward: -0.0661 | Final Net Worth: $7,517.69 | Epsilon: 1.00%\n",
            "Episode: 240/500 | Steps: 18 | Avg Reward: -0.1323 | Final Net Worth: $10,082.27 | Epsilon: 1.00%\n",
            "Episode: 250/500 | Steps: 49 | Avg Reward: -0.0622 | Final Net Worth: $10,558.41 | Epsilon: 1.00%\n",
            "Episode: 260/500 | Steps: 405 | Avg Reward: -0.1296 | Final Net Worth: $8,788.68 | Epsilon: 1.00%\n",
            "Episode: 270/500 | Steps: 404 | Avg Reward: -0.0927 | Final Net Worth: $8,480.43 | Epsilon: 1.00%\n",
            "Episode: 280/500 | Steps: 417 | Avg Reward: -0.1002 | Final Net Worth: $7,318.61 | Epsilon: 1.00%\n",
            "Episode: 290/500 | Steps: 78 | Avg Reward: -0.1419 | Final Net Worth: $10,014.27 | Epsilon: 1.00%\n",
            "Episode: 300/500 | Steps: 327 | Avg Reward: -0.1009 | Final Net Worth: $8,508.85 | Epsilon: 1.00%\n",
            "Episode: 310/500 | Steps: 63 | Avg Reward: -0.1024 | Final Net Worth: $10,344.21 | Epsilon: 1.00%\n",
            "Episode: 320/500 | Steps: 440 | Avg Reward: -0.1459 | Final Net Worth: $6,987.08 | Epsilon: 1.00%\n",
            "Episode: 330/500 | Steps: 132 | Avg Reward: -0.1272 | Final Net Worth: $9,008.31 | Epsilon: 1.00%\n",
            "Episode: 340/500 | Steps: 419 | Avg Reward: -0.0907 | Final Net Worth: $8,532.27 | Epsilon: 1.00%\n",
            "Episode: 350/500 | Steps: 124 | Avg Reward: -0.0445 | Final Net Worth: $9,688.42 | Epsilon: 1.00%\n",
            "Episode: 360/500 | Steps: 35 | Avg Reward: -0.1454 | Final Net Worth: $10,422.39 | Epsilon: 1.00%\n",
            "Episode: 370/500 | Steps: 78 | Avg Reward: -0.0812 | Final Net Worth: $9,611.47 | Epsilon: 1.00%\n",
            "Episode: 380/500 | Steps: 387 | Avg Reward: -0.1361 | Final Net Worth: $7,694.78 | Epsilon: 1.00%\n",
            "Episode: 390/500 | Steps: 202 | Avg Reward: -0.1208 | Final Net Worth: $9,006.98 | Epsilon: 1.00%\n",
            "Episode: 400/500 | Steps: 165 | Avg Reward: -0.1745 | Final Net Worth: $8,420.99 | Epsilon: 1.00%\n",
            "Episode: 410/500 | Steps: 376 | Avg Reward: -0.1303 | Final Net Worth: $8,440.23 | Epsilon: 1.00%\n",
            "Episode: 420/500 | Steps: 116 | Avg Reward: -0.1112 | Final Net Worth: $9,673.08 | Epsilon: 1.00%\n",
            "Episode: 430/500 | Steps: 460 | Avg Reward: -0.1361 | Final Net Worth: $7,181.47 | Epsilon: 1.00%\n",
            "Episode: 440/500 | Steps: 357 | Avg Reward: -0.1391 | Final Net Worth: $6,764.78 | Epsilon: 1.00%\n",
            "Episode: 450/500 | Steps: 457 | Avg Reward: -0.1451 | Final Net Worth: $7,882.38 | Epsilon: 1.00%\n",
            "Episode: 460/500 | Steps: 431 | Avg Reward: -0.1059 | Final Net Worth: $8,000.78 | Epsilon: 1.00%\n",
            "Episode: 470/500 | Steps: 362 | Avg Reward: -0.0632 | Final Net Worth: $10,419.21 | Epsilon: 1.00%\n",
            "Episode: 480/500 | Steps: 183 | Avg Reward: -0.1497 | Final Net Worth: $8,468.43 | Epsilon: 1.00%\n",
            "Episode: 490/500 | Steps: 311 | Avg Reward: -0.1103 | Final Net Worth: $7,755.92 | Epsilon: 1.00%\n",
            "Episode: 500/500 | Steps: 88 | Avg Reward: -0.0838 | Final Net Worth: $9,408.42 | Epsilon: 1.00%\n",
            "\n",
            "DQN 訓練完成，模型已保存至 ./gateio_dqn_model.pth\n"
          ]
        }
      ]
    }
  ]
}